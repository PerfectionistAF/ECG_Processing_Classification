{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2916cfb",
   "metadata": {},
   "source": [
    "# Optmiziation of Deep Learning for Cardiologist-level Myocardial Infarction Detection in Electrocardiograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bbd4d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Many neural networks we tested: AlexNet, ResNet50, Gradient Boosting CNN, ConvNetQuake\n",
    "## the best one so far is the ConvQuakeNet\n",
    "\n",
    "##start by processing the data\n",
    "##then create the nn.Model subclass\n",
    "##then call the Pso(forward function of model)\n",
    "##then return loss score; ie optimize hyperparameters for search convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d6bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Downloads\\ANACONDA3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\Downloads\\ANACONDA3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "E:\\Downloads\\ANACONDA3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "E:\\Downloads\\ANACONDA3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "##extract data in dataframes, use 2 channels, v6: anterior lead, vz: left wrist lead\n",
    "##two most prominent in myocardial infarction diagnosis\n",
    "## no cleaning required, data is clean\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import wfdb\n",
    "import time\n",
    "import random\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import sys\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b676ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, MSELoss\n",
    "from torch_pso import ParticleSwarmOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065c6c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362d7089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "922dd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "289685fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21dd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD  ##use the 2 apis PyTorch and KerasCV, KerasCV is easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8258dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##experiment with seed, run, channels(sinoatrial (SA) node, left feature lead, septal)\n",
    "##later experiment with channel averaging of physician diagnosis features of myocardial infarction\n",
    "##(septal average: v1, v2 ; left anterior average: v5, v6; left wrist: avl)\n",
    "seed_num = 39 #sys.argv[1]  ##required bash files\n",
    "#run_num = #sys.argv[2]\n",
    "channel_1 = 'v6'#sys.argv[3]\n",
    "channel_2 = 'vz'#sys.argv[4]\n",
    "channel_3 = 'ii'#sys.argv[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6f4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/RECORDS') as fp:  \n",
    "    lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9cabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines  ##display lines in records file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f5a7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_myocardial = []\n",
    "files_healthy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3034086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in lines:\n",
    "    file_path = './ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1] + '.hea'\n",
    "    \n",
    "    ##read header to determine class ##focus on myocardial infarction\n",
    "    if 'Myocardial infarction' in open(file_path).read():\n",
    "        files_myocardial.append(file)\n",
    "        \n",
    "    if 'Healthy control' in open(file_path).read():\n",
    "        files_healthy.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2642ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##shuffle data (cross-validation)\n",
    "np.random.seed(int(seed_num))\n",
    "np.random.shuffle(files_myocardial) \n",
    "np.random.shuffle(files_healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b41c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patient240/s0468_re\\n',\n",
       " 'patient246/s0478_re\\n',\n",
       " 'patient150/s0287lre\\n',\n",
       " 'patient233/s0482_re\\n',\n",
       " 'patient244/s0473_re\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_healthy[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba90a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files_healthy  ##check with CONTROL\n",
    "#files_myocardial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c0b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##split to train and test, 80% train, 20% test\n",
    "healthy_train = files_healthy[:int(0.8*len(files_healthy))]\n",
    "healthy_val = files_healthy[int(0.8*len(files_healthy)):]\n",
    "myocardial_train = files_myocardial[:int(0.8*len(files_myocardial))]\n",
    "myocardial_val = files_myocardial[int(0.8*len(files_myocardial)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "830c3b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patient240/s0468_re\\n',\n",
       " 'patient246/s0478_re\\n',\n",
       " 'patient150/s0287lre\\n',\n",
       " 'patient233/s0482_re\\n',\n",
       " 'patient244/s0473_re\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f0a4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1043e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids_healthy_train = []\n",
    "patient_ids_healthy_val = []\n",
    "patient_ids_myocardial_train = []\n",
    "patient_ids_myocardial_val = []\n",
    "##extract first 10 letters in file path string, which is the patient id\n",
    "for index in healthy_train:\n",
    "    patient_ids_healthy_train.append(index[0:10])\n",
    "for index in healthy_val:\n",
    "    patient_ids_healthy_val.append(index[0:10])\n",
    "for index in myocardial_train:\n",
    "    patient_ids_myocardial_train.append(index[0:10])\n",
    "for index in myocardial_val:\n",
    "    patient_ids_myocardial_val.append(index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "166b9909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(patient_ids_myocardial_train)  ##list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c3962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(patient_ids_myocardial_train)) ##80% of total 148 patients is 118  \n",
    "#likely to have duplicates, so distribute intersection over train and validation sets\n",
    "#they can have common ecgs, used for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abff5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(patient_ids_myocardial_val)) ##20% of total 148 patients is 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc9bd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patient_ids_myocardial_val##stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07584f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_myocardial = intersection(patient_ids_myocardial_train, patient_ids_myocardial_val)\n",
    "intersection_healthy = intersection(patient_ids_healthy_train, patient_ids_healthy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2541e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "##myocardial \n",
    "move_to_train = intersection_myocardial[:int(0.5*len(intersection_myocardial))]\n",
    "move_to_val = intersection_myocardial[int(0.5*len(intersection_myocardial)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b34f2d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patient223', 'patient017', 'patient100', 'patient080', 'patient045']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_to_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d55dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in move_to_train:\n",
    "    in_val = []\n",
    "    ##find and remove all files in val ecg readings by id\n",
    "    for file_ in myocardial_val:\n",
    "        if file_[:10] == patient_id:\n",
    "            in_val.append(file_)\n",
    "            myocardial_val.remove(file_)\n",
    "            \n",
    "    ##add to train\n",
    "    for file_ in in_val:\n",
    "        myocardial_train.append(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d779416",
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in move_to_val:    \n",
    "    in_train = []\n",
    "    ##find and remove all files in train\n",
    "    for file_ in myocardial_train:\n",
    "        if file_[:10] == patient_id:\n",
    "            in_train.append(file_)\n",
    "            myocardial_train.remove(file_)\n",
    "            \n",
    "    ##add to val\n",
    "    for file_ in in_train:\n",
    "        myocardial_val.append(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7799a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient251', 'patient245', 'patient233']\n"
     ]
    }
   ],
   "source": [
    "##healthy\n",
    "move_to_train = intersection_healthy[:int(0.5*len(intersection_healthy))]\n",
    "move_to_val = intersection_healthy[int(0.5*len(intersection_healthy)):]\n",
    "print(move_to_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16fe8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in move_to_train:\n",
    "    in_val = []\n",
    "    ##find and remove all files in val ecg readings by id\n",
    "    for file_ in healthy_val:\n",
    "        if file_[:10] == patient_id:\n",
    "            in_val.append(file_)\n",
    "            healthy_val.remove(file_)\n",
    "            \n",
    "    ##add to train\n",
    "    for file_ in in_val:\n",
    "        healthy_train.append(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01ec3c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in move_to_val:    \n",
    "    in_train = []\n",
    "    ##find and remove all files in train\n",
    "    for file_ in healthy_train:\n",
    "        if file_[:10] == patient_id:\n",
    "            in_train.append(file_)\n",
    "            healthy_train.remove(file_)\n",
    "            \n",
    "    ##add to val\n",
    "    for file_ in in_train:\n",
    "        healthy_val.append(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68d718b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(myocardial_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4a50dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patient240/s0468_re'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy_train[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afa68d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy_train = []\n",
    "for file in healthy_train:\n",
    "    ##records for each\n",
    "    data_ii, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_1)])\n",
    "    data_v6, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_2)])\n",
    "    data_vz, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_3)])\n",
    "    data = [data_ii.flatten(), data_v6.flatten(), data_vz.flatten()]  ##flatten to input directly into keras model\n",
    "    data_healthy_train.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "771fcf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.1245, -0.1275, -0.1305, ...,  0.188 ,  0.187 ,  0.1855]),\n",
       " array([ 0.458 ,  0.458 ,  0.457 , ..., -0.335 , -0.3325, -0.329 ]),\n",
       " array([-0.1025, -0.1   , -0.0925, ...,  0.062 ,  0.0695,  0.065 ])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_healthy_train[0]##array output of each of the three channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b35e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_healthy_val = []\n",
    "for file in healthy_val:\n",
    "    ##records for each\n",
    "    data_ii, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_1)])\n",
    "    data_v6, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_2)])\n",
    "    data_vz, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_3)])\n",
    "    data = [data_ii.flatten(), data_v6.flatten(), data_vz.flatten()]  ##flatten to input directly into keras model\n",
    "    data_healthy_val.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "599f4b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.1595, -0.1605, -0.164 , ...,  0.106 ,  0.1025,  0.0995]),\n",
       " array([-0.008 , -0.007 , -0.0065, ..., -0.074 , -0.0725, -0.071 ]),\n",
       " array([-0.0795, -0.0805, -0.082 , ...,  0.046 ,  0.0445,  0.042 ])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_healthy_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91953a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unhealthy_train = []\n",
    "for file in myocardial_train:\n",
    "    ##records for each\n",
    "    data_ii, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_1)])\n",
    "    data_v6, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_2)])\n",
    "    data_vz, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_3)])\n",
    "    data = [data_ii.flatten(), data_v6.flatten(), data_vz.flatten()]  ##flatten to input directly into keras model\n",
    "    data_unhealthy_train.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "821528b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unhealthy_val = []\n",
    "for file in myocardial_val:\n",
    "    ##records for each\n",
    "    data_ii, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_1)])\n",
    "    data_v6, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_2)])\n",
    "    data_vz, _ = wfdb.rdsamp('./ptb-diagnostic-ecg-database-1.0.0/ptb-diagnostic-ecg-database-1.0.0/' + file[:-1], channel_names=[str(channel_3)])\n",
    "    data = [data_ii.flatten(), data_v6.flatten(), data_vz.flatten()]  ##flatten to input directly into keras model\n",
    "    data_unhealthy_val.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b51bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_14992\\2361096082.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_healthy_train = np.asarray(data_healthy_train)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_14992\\2361096082.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_healthy_val = np.asarray(data_healthy_val)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_14992\\2361096082.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_unhealthy_train = np.asarray(data_unhealthy_train)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_14992\\2361096082.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_unhealthy_val = np.asarray(data_unhealthy_val)\n"
     ]
    }
   ],
   "source": [
    "##check they are all arrays\n",
    "data_healthy_train = np.asarray(data_healthy_train)\n",
    "data_healthy_val = np.asarray(data_healthy_val)\n",
    "data_unhealthy_train = np.asarray(data_unhealthy_train)\n",
    "data_unhealthy_val = np.asarray(data_unhealthy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f11bf463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([-0.1245, -0.1275, -0.1305, ...,  0.188 ,  0.187 ,  0.1855]),\n",
       "       array([ 0.458 ,  0.458 ,  0.457 , ..., -0.335 , -0.3325, -0.329 ]),\n",
       "       array([-0.1025, -0.1   , -0.0925, ...,  0.062 ,  0.0695,  0.065 ])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_healthy_train[0] ##true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b43c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10000  ##from each ecg reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d6aa02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, split='train'):\n",
    "    ##random sampling of batches\n",
    "    ##divide batch number in half to improve speed of network\n",
    "    if split == 'train':\n",
    "        unhealthy_indices = random.sample(sorted(np.arange(len(data_unhealthy_train))), k=int(batch_size / 2))\n",
    "        healthy_indices = random.sample(sorted(np.arange(len(data_healthy_train))), k=int(batch_size / 2))\n",
    "        unhealthy_batch = data_unhealthy_train[unhealthy_indices]\n",
    "        healthy_batch = data_healthy_train[healthy_indices]\n",
    "    elif split == 'val': \n",
    "        unhealthy_indices = random.sample(sorted(np.arange(len(data_unhealthy_val))), k=int(batch_size / 2))\n",
    "        healthy_indices = random.sample(sorted(np.arange(len(data_healthy_val))), k=int(batch_size / 2))\n",
    "        unhealthy_batch = data_unhealthy_val[unhealthy_indices]\n",
    "        healthy_batch = data_healthy_val[healthy_indices]\n",
    "    \n",
    "    batch_x = []  ##batch of mixed healthy and unhealthy data\n",
    "    for sample in unhealthy_batch: ##if val or if train\n",
    "        start = random.choice(np.arange(len(sample[0]) - window_size))  ##randomly sample window from ecg \n",
    "        # normalize ecg values \n",
    "        normalized_1 = minmax_scale(sample[0][start:start+window_size])\n",
    "        normalized_2 = minmax_scale(sample[1][start:start+window_size])\n",
    "        normalized_3 = minmax_scale(sample[2][start:start+window_size])\n",
    "        normalized = np.array((normalized_1, normalized_2, normalized_3))\n",
    "        batch_x.append(normalized)\n",
    "        \n",
    "    for sample in healthy_batch:\n",
    "        start = random.choice(np.arange(len(sample[0]) - window_size))\n",
    "        # normalize\n",
    "        normalized_1 = minmax_scale(sample[0][start:start+window_size])\n",
    "        normalized_2 = minmax_scale(sample[1][start:start+window_size])\n",
    "        normalized_3 = minmax_scale(sample[2][start:start+window_size])\n",
    "        normalized = np.array((normalized_1, normalized_2, normalized_3))\n",
    "        batch_x.append(normalized)\n",
    "    \n",
    "    batch_y = [0.1 for _ in range(int(batch_size / 2))]\n",
    "    for _ in range(int(batch_size / 2)):\n",
    "        batch_y.append(0.9)\n",
    "        \n",
    "    indices = np.arange(len(batch_y))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_y = np.array(batch_y)\n",
    "    \n",
    "    batch_x = batch_x[indices]\n",
    "    batch_y = batch_y[indices]\n",
    "    \n",
    "    batch_x = np.reshape(batch_x, (-1, 3, window_size))\n",
    "    batch_x = torch.from_numpy(batch_x)\n",
    "    batch_x = batch_x.float()#.cuda()\n",
    "    batch_x = batch_x.float()\n",
    "    \n",
    "    batch_y = np.reshape(batch_y, (-1, 1))\n",
    "    batch_y = torch.from_numpy(batch_y)\n",
    "    batch_y = batch_y.float()#.cuda()\n",
    "    batch_y = batch_y.float()\n",
    "    \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2b6ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "##keras sequential model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bea2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetQuake(nn.Module):##issues with keras attributes, use PyTorch\n",
    "    def __init__(self):\n",
    "        super(ConvNetQuake, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv7 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv8 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.linear1 = nn.Linear(1280, 128)\n",
    "        self.linear2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "        self.bn7 = nn.BatchNorm1d(32)\n",
    "        self.bn8 = nn.BatchNorm1d(32)\n",
    "        '''self.conv1 = keras.layers.Conv1D(kernel_size=3, stride=2, padding=1, data_format=(batch, features, steps))(x)\n",
    "        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n",
    "        self.dropout = keras.layers.Dropout(0.5)'''\n",
    "\n",
    "    def forward(self, x):  ##x is preliminary input\n",
    "        x = self.bn1(F.relu((self.conv1(x))))\n",
    "        x = self.bn2(F.relu((self.conv2(x))))\n",
    "        x = self.bn3(F.relu((self.conv3(x))))\n",
    "        x = self.bn4(F.relu((self.conv4(x))))\n",
    "        x = self.bn5(F.relu((self.conv5(x))))\n",
    "        x = self.bn6(F.relu((self.conv6(x))))\n",
    "        x = self.bn7(F.relu((self.conv7(x))))\n",
    "        x = self.bn8(F.relu((self.conv8(x))))\n",
    "        x = torch.reshape(x, (10, -1))\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        '''x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.dense2(x)'''\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''net = Sequential(Linear(10,100), Linear(100,100), Linear(100,10))\n",
    "optim = ParticleSwarmOptimizer(net.parameters(),\n",
    "                               inertial_weight=0.5,\n",
    "                               num_particles=100,\n",
    "                               max_param_value=1,\n",
    "                               min_param_value=-1)\n",
    "criterion = MSELoss()\n",
    "target = torch.rand((10,)).round()\n",
    "\n",
    "x = torch.rand((10,))\n",
    "for _ in range(100):\n",
    "    \n",
    "    def closure():\n",
    "        # Clear any grads from before the optimization step, since we will be changing the parameters\n",
    "        optim.zero_grad()  \n",
    "        return criterion(net(x), target)\n",
    "    \n",
    "    optim.step(closure)\n",
    "    print('Prediciton', net(x))\n",
    "    print('Target    ', target)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c1a97ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train tensor(50., grad_fn=<BinaryCrossEntropyBackward0>) 100\n",
      "Accuracy/val 48.1 100\n",
      "Accuracy/train 48.3 100\n",
      "Loss/train tensor(41.5681, grad_fn=<BinaryCrossEntropyBackward0>) 200\n",
      "Accuracy/val 48.8 200\n",
      "Accuracy/train 49.3 200\n",
      "Loss/train tensor(58., grad_fn=<BinaryCrossEntropyBackward0>) 300\n",
      "Accuracy/val 49.2 300\n",
      "Accuracy/train 48.4 300\n",
      "Loss/train tensor(45.3223, grad_fn=<BinaryCrossEntropyBackward0>) 400\n",
      "Accuracy/val 47.3 400\n",
      "Accuracy/train 48.0 400\n",
      "Loss/train tensor(49.2593, grad_fn=<BinaryCrossEntropyBackward0>) 500\n",
      "Accuracy/val 48.7 500\n",
      "Accuracy/train 47.6 500\n",
      "Loss/train tensor(41.1650, grad_fn=<BinaryCrossEntropyBackward0>) 600\n",
      "Accuracy/val 48.3 600\n",
      "Accuracy/train 47.2 600\n",
      "Loss/train tensor(31.2326, grad_fn=<BinaryCrossEntropyBackward0>) 700\n",
      "Accuracy/val 48.2 700\n",
      "Accuracy/train 47.4 700\n",
      "Loss/train tensor(41.7382, grad_fn=<BinaryCrossEntropyBackward0>) 800\n",
      "Accuracy/val 48.7 800\n",
      "Accuracy/train 47.2 800\n",
      "Loss/train tensor(35.8350, grad_fn=<BinaryCrossEntropyBackward0>) 900\n",
      "Accuracy/val 50.5 900\n",
      "Accuracy/train 46.4 900\n",
      "Loss/train tensor(42.1240, grad_fn=<BinaryCrossEntropyBackward0>) 1000\n",
      "Accuracy/val 48.5 1000\n",
      "Accuracy/train 48.4 1000\n",
      "Loss/train tensor(49.6257, grad_fn=<BinaryCrossEntropyBackward0>) 1100\n",
      "Accuracy/val 48.6 1100\n",
      "Accuracy/train 47.2 1100\n",
      "Loss/train tensor(52.0220, grad_fn=<BinaryCrossEntropyBackward0>) 1200\n",
      "Accuracy/val 47.3 1200\n",
      "Accuracy/train 49.4 1200\n",
      "Loss/train tensor(43.2542, grad_fn=<BinaryCrossEntropyBackward0>) 1300\n",
      "Accuracy/val 47.5 1300\n",
      "Accuracy/train 48.7 1300\n",
      "Loss/train tensor(51.0039, grad_fn=<BinaryCrossEntropyBackward0>) 1400\n",
      "Accuracy/val 45.7 1400\n",
      "Accuracy/train 48.1 1400\n",
      "Loss/train tensor(41.5448, grad_fn=<BinaryCrossEntropyBackward0>) 1500\n",
      "Accuracy/val 47.5 1500\n",
      "Accuracy/train 47.7 1500\n",
      "Loss/train tensor(48.7661, grad_fn=<BinaryCrossEntropyBackward0>) 1600\n",
      "Accuracy/val 48.5 1600\n",
      "Accuracy/train 46.5 1600\n",
      "Loss/train tensor(49.0567, grad_fn=<BinaryCrossEntropyBackward0>) 1700\n",
      "Accuracy/val 47.0 1700\n",
      "Accuracy/train 47.6 1700\n",
      "Loss/train tensor(50., grad_fn=<BinaryCrossEntropyBackward0>) 1800\n",
      "Accuracy/val 49.5 1800\n",
      "Accuracy/train 47.6 1800\n",
      "Loss/train tensor(39.8190, grad_fn=<BinaryCrossEntropyBackward0>) 1900\n",
      "Accuracy/val 48.8 1900\n",
      "Accuracy/train 47.6 1900\n",
      "Loss/train tensor(34., grad_fn=<BinaryCrossEntropyBackward0>) 2000\n",
      "Accuracy/val 51.8 2000\n",
      "Accuracy/train 46.7 2000\n",
      "Loss/train tensor(25.6255, grad_fn=<BinaryCrossEntropyBackward0>) 2100\n",
      "Accuracy/val 47.9 2100\n",
      "Accuracy/train 47.6 2100\n",
      "Loss/train tensor(56.5929, grad_fn=<BinaryCrossEntropyBackward0>) 2200\n",
      "Accuracy/val 49.4 2200\n",
      "Accuracy/train 46.5 2200\n",
      "Loss/train tensor(49.8336, grad_fn=<BinaryCrossEntropyBackward0>) 2300\n",
      "Accuracy/val 48.0 2300\n",
      "Accuracy/train 45.3 2300\n",
      "Loss/train tensor(58., grad_fn=<BinaryCrossEntropyBackward0>) 2400\n",
      "Accuracy/val 48.7 2400\n",
      "Accuracy/train 45.6 2400\n",
      "Loss/train tensor(49.9713, grad_fn=<BinaryCrossEntropyBackward0>) 2500\n",
      "Accuracy/val 50.1 2500\n",
      "Accuracy/train 48.5 2500\n",
      "Loss/train tensor(43.8885, grad_fn=<BinaryCrossEntropyBackward0>) 2600\n",
      "Accuracy/val 49.1 2600\n",
      "Accuracy/train 46.5 2600\n",
      "Loss/train tensor(48.0431, grad_fn=<BinaryCrossEntropyBackward0>) 2700\n",
      "Accuracy/val 47.8 2700\n",
      "Accuracy/train 47.1 2700\n",
      "Loss/train tensor(46.6836, grad_fn=<BinaryCrossEntropyBackward0>) 2800\n",
      "Accuracy/val 49.8 2800\n",
      "Accuracy/train 48.1 2800\n",
      "Loss/train tensor(42.0014, grad_fn=<BinaryCrossEntropyBackward0>) 2900\n",
      "Accuracy/val 48.6 2900\n",
      "Accuracy/train 47.4 2900\n",
      "Loss/train tensor(58., grad_fn=<BinaryCrossEntropyBackward0>) 3000\n",
      "Accuracy/val 49.1 3000\n",
      "Accuracy/train 47.2 3000\n",
      "Loss/train tensor(41.1075, grad_fn=<BinaryCrossEntropyBackward0>) 3100\n",
      "Accuracy/val 46.6 3100\n",
      "Accuracy/train 47.1 3100\n",
      "Loss/train tensor(58., grad_fn=<BinaryCrossEntropyBackward0>) 3200\n",
      "Accuracy/val 49.1 3200\n",
      "Accuracy/train 49.0 3200\n",
      "Loss/train tensor(49.1013, grad_fn=<BinaryCrossEntropyBackward0>) 3300\n",
      "Accuracy/val 50.4 3300\n",
      "Accuracy/train 48.1 3300\n",
      "Loss/train tensor(50., grad_fn=<BinaryCrossEntropyBackward0>) 3400\n",
      "Accuracy/val 48.1 3400\n",
      "Accuracy/train 44.4 3400\n",
      "Loss/train tensor(57.4633, grad_fn=<BinaryCrossEntropyBackward0>) 3500\n",
      "Accuracy/val 49.0 3500\n",
      "Accuracy/train 47.3 3500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iters \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m     20\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m get_batch(batch_size, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m():\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# Clear any grads from before the optimization step, since we will be changing the parameters\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[1;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 32\u001b[0m, in \u001b[0;36mConvNetQuake.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(F\u001b[38;5;241m.\u001b[39mrelu((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(F\u001b[38;5;241m.\u001b[39mrelu((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x))))\n\u001b[1;32m---> 32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn6(F\u001b[38;5;241m.\u001b[39mrelu((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv6(x))))\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn7(F\u001b[38;5;241m.\u001b[39mrelu((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv7(x))))\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:155\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    157\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ConvNetQuake()\n",
    "#model.cuda() ##compute engine device\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=[0])\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-4)\n",
    "optimizer = ParticleSwarmOptimizer(model.parameters(),\n",
    "                               inertial_weight=0.5,\n",
    "                               num_particles=100,\n",
    "                               max_param_value=1,\n",
    "                               min_param_value=-1)\n",
    "criterion = nn.BCELoss()\n",
    "##run the model now\n",
    "num_iters = 50000\n",
    "batch_size = 10\n",
    "\n",
    "acc_values = []\n",
    "acc_values_train = []\n",
    "\n",
    "for iters in range(num_iters):\n",
    "    batch_x, batch_y = get_batch(batch_size, split='train')\n",
    "    y_pred = model(batch_x)\n",
    "    \n",
    "    def closure():\n",
    "        # Clear any grads from before the optimization step, since we will be changing the parameters\n",
    "        optimizer.zero_grad()  \n",
    "        return criterion(y_pred, batch_y)\n",
    "    \n",
    "    loss = criterion(y_pred, batch_y)\n",
    "    #optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # validation\n",
    "    if iters%100 == 0 and iters != 0:\n",
    "\n",
    "        print('Loss/train', loss, iters)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # test_set\n",
    "            iterations = 100\n",
    "            avg_acc = 0\n",
    "\n",
    "            for _ in range(iterations):\n",
    "                batch_x, batch_y = get_batch(batch_size, split='val')\n",
    "                cleaned = model(batch_x)\n",
    "\n",
    "                count = 0\n",
    "                acc = 0\n",
    "                for num in cleaned:\n",
    "                    if int(torch.round(num)) == int(torch.round(batch_y[count])):\n",
    "                        acc += 10\n",
    "                    count += 1\n",
    "                avg_acc += acc\n",
    "\n",
    "            acc_values.append((avg_acc / iterations))\n",
    "            print('Accuracy/val', (avg_acc / iterations), iters)\n",
    "\n",
    "            # train_set\n",
    "            iterations = 100\n",
    "            avg_acc = 0\n",
    "\n",
    "            for _ in range(iterations):\n",
    "                batch_x, batch_y = get_batch(batch_size, split='train')\n",
    "                cleaned = model(batch_x)\n",
    "\n",
    "                count = 0\n",
    "                acc = 0\n",
    "                for num in cleaned:\n",
    "                    if int(torch.round(num)) == int(torch.round(batch_y[count])):\n",
    "                        acc += 10\n",
    "                    count += 1\n",
    "                avg_acc += acc\n",
    "\n",
    "            acc_values_train.append((avg_acc / iterations))\n",
    "            print('Accuracy/train', (avg_acc / iterations), iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4848a31",
   "metadata": {},
   "source": [
    "## _______________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c19951c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PSO Classes\n",
    "##Particle object class\n",
    "class Particle(object):\n",
    "    '''Particle class for PSO: each search member is a particle'''\n",
    "\n",
    "    '''Arguments:\n",
    "        lower_bound (np.array): Vector of lower boundaries for particle dimensions.\n",
    "        upper_bound (np.array): Vector of upper boundaries for particle dimensions.\n",
    "        dimensions (int): Number of dimensions of the search space.\n",
    "        objective function (function): Black-box function to evaluate.\n",
    "'''\n",
    "    def __init__(self, lower_bound, upper_bound, dimensions, objective_function):\n",
    "        self.reset(dimensions, lower_bound, upper_bound, objective_function)\n",
    "\n",
    "    def reset(self, dimensions, lower_bound, upper_bound, objective_function):\n",
    "        '''Particle reset: allows for reset of a particle without reallocation'''\n",
    "        position = []\n",
    "        for i in range(dimensions):\n",
    "            if lower_bound[i] < upper_bound[i]:\n",
    "                position.extend(np.random.randint(lower_bound[i], upper_bound[i] + 1, 1, dtype=int))\n",
    "            elif lower_bound[i] == upper_bound[i]:\n",
    "                position.extend(np.array([lower_bound[i]], dtype=int))\n",
    "            else:\n",
    "                assert False\n",
    "\n",
    "        self.position = [position]\n",
    "\n",
    "        self.velocity = [np.multiply(np.random.rand(dimensions),\n",
    "                                     (upper_bound - lower_bound)).astype(int)]\n",
    "\n",
    "        self.best_position = self.position[:]\n",
    "\n",
    "        self.function_value = [objective_function(self.best_position[-1])]\n",
    "        self.best_function_value = self.function_value[:]\n",
    "    \n",
    "    def update_velocity(self, omega, phip, phig, best_swarm_position):\n",
    "        '''Particle velocity update'''\n",
    "\n",
    "        '''Args:\n",
    "            omega (float): Velocity equation constant.\n",
    "            phip (float): Velocity equation constant.\n",
    "            phig (float): Velocity equation constant.\n",
    "            best_swarm_position (np.array): Best global particle position.'''\n",
    "\n",
    "        random_coefficient_p = np.random.uniform(size=np.asarray(self.position[-1]).shape)\n",
    "        random_coefficient_g = np.random.uniform(size=np.asarray(self.position[-1]).shape)\n",
    "\n",
    "        self.velocity.append(omega*np.asarray(self.velocity[-1])\n",
    "                             + (phip*random_coefficient_p\n",
    "                                * (np.asarray(self.best_position[-1])- np.asarray(self.position[-1])))\n",
    "                             + (phig* random_coefficient_g\n",
    "                             * (np.asarray(best_swarm_position)- np.asarray(self.position[-1]))))\n",
    "\n",
    "        self.velocity[-1] = self.velocity[-1].astype(int)\n",
    "\n",
    "    def update_position(self, lower_bound, upper_bound, objective_function):\n",
    "        '''Particle position update, current position + Vd+1'''\n",
    "                             \n",
    "        new_position = self.position[-1] + self.velocity[-1]\n",
    "\n",
    "        if np.array_equal(self.position[-1], new_position):\n",
    "            self.function_value.append(self.function_value[-1])\n",
    "        else:\n",
    "            mark1 = new_position < lower_bound\n",
    "            mark2 = new_position > upper_bound\n",
    "\n",
    "            new_position[mark1] = lower_bound[mark1]\n",
    "            new_position[mark2] = upper_bound[mark2]\n",
    "\n",
    "            self.function_value.append(objective_function(self.position[-1]))\n",
    "\n",
    "        self.position.append(new_position.tolist())\n",
    "\n",
    "        if self.function_value[-1] < self.best_function_value[-1]:\n",
    "            self.best_position.append(self.position[-1][:])\n",
    "            self.best_function_value.append(self.function_value[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bc2069b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pso(object):\n",
    "    '''PSO wrapper\n",
    "\n",
    "    This class contains the particles and provides an abstraction to hold all the context\n",
    "    of the PSO algorithm\n",
    "\n",
    "    Args:\n",
    "        swarmsize (int): Number of particles in the swarm\n",
    "        maxiter (int): Maximum number of generations the swarm will run'''\n",
    "\n",
    "    \n",
    "    def __init__(self, swarmsize=100, maxiter=100):\n",
    "        self.max_generations = maxiter\n",
    "        self.swarmsize = swarmsize\n",
    "\n",
    "        self.omega = 0.5  ##start c1 and c2 at 0.5, balance exploration and exploitation\n",
    "        self.phip = 0.5\n",
    "        self.phig = 0.5\n",
    "\n",
    "        self.minstep = 1e-4\n",
    "        self.minfunc = 1e-4\n",
    "\n",
    "        self.best_position = [None]\n",
    "        self.best_function_value = [1]\n",
    "\n",
    "        self.particles = []\n",
    "\n",
    "        self.retired_particles = []\n",
    "\n",
    "    def run(self, function, lower_bound, upper_bound, kwargs=None):\n",
    "        '''Perform a particle swarm optimization (PSO)'''\n",
    "\n",
    "        '''Args:\n",
    "            objective_function (function): The function to be minimized.\n",
    "            lower_bound (np.array): Vector of lower boundaries for particle dimensions.\n",
    "            upper_bound (np.array): Vector of upper boundaries for particle dimensions.\n",
    "\n",
    "        Returns:\n",
    "            best_position (np.array): Best known position\n",
    "            accuracy (float): Objective value at best_position\n",
    "            :param kwargs:'''\n",
    "\n",
    "        \n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "\n",
    "        objective_function = lambda x: function(x, **kwargs)\n",
    "        assert hasattr(function, '__call__'), 'Invalid function handle'\n",
    "\n",
    "        assert len(lower_bound) == len(upper_bound), 'Invalid bounds length'\n",
    "\n",
    "        lower_bound = np.array(lower_bound)\n",
    "        upper_bound = np.array(upper_bound)\n",
    "\n",
    "        assert np.all(upper_bound > lower_bound), 'Invalid boundary values'\n",
    "\n",
    "\n",
    "        dimensions = len(lower_bound)\n",
    "\n",
    "        self.particles = self.initialize_particles(lower_bound,\n",
    "                                                   upper_bound,\n",
    "                                                   dimensions,\n",
    "                                                   objective_function)\n",
    "\n",
    "        #Start evolution\n",
    "        generation = 1\n",
    "        while generation <= self.max_generations:\n",
    "            for particle in self.particles:\n",
    "                particle.update_velocity(self.omega, self.phip, self.phig, self.best_position[-1])\n",
    "                particle.update_position(lower_bound, upper_bound, objective_function)\n",
    "\n",
    "                if particle.best_function_value[-1] == 0:\n",
    "                    self.retired_particles.append(copy.deepcopy(particle))\n",
    "                    particle.reset(dimensions, lower_bound, upper_bound, objective_function)\n",
    "                elif particle.best_function_value[-1] < self.best_function_value[-1]:\n",
    "                    stepsize = np.sqrt(np.sum((np.asarray(self.best_position[-1])\n",
    "                                               - np.asarray(particle.position[-1])) ** 2))\n",
    "\n",
    "                    if np.abs(np.asarray(self.best_function_value[-1])\n",
    "                              - np.asarray(particle.best_function_value[-1])) \\\n",
    "                            <= self.minfunc:\n",
    "                        return particle.best_position[-1], particle.best_function_value[-1]\n",
    "                    elif stepsize <= self.minstep:\n",
    "                        return particle.best_position[-1], particle.best_function_value[-1]\n",
    "                    else:\n",
    "                        self.best_function_value.append(particle.best_function_value[-1])\n",
    "                        self.best_position.append(particle.best_position[-1][:])\n",
    "\n",
    "\n",
    "\n",
    "            generation += 1\n",
    "\n",
    "        return self.best_position[-1], self.best_function_value[-1]\n",
    "\n",
    "    def initialize_particles(self, lower_bound, upper_bound, dimensions, objective_function):\n",
    "        '''Initializes the particles for the swarm'''\n",
    "\n",
    "        '''Args:\n",
    "            objective_function (function): The function to be minimized.\n",
    "            lower_bound (np.array): Vector of lower boundaries for particle dimensions.\n",
    "            upper_bound (np.array): Vector of upper boundaries for particle dimensions.\n",
    "            dimensions (int): Number of dimensions of the search space.\n",
    "\n",
    "        Returns:\n",
    "            particles (list): Collection or particles in the swarm'''\n",
    "        \n",
    "        particles = []\n",
    "        for _ in range(self.swarmsize):\n",
    "            particles.append(Particle(lower_bound,\n",
    "                                      upper_bound,\n",
    "                                      dimensions,\n",
    "                                      objective_function))\n",
    "            if particles[-1].best_function_value[-1] < self.best_function_value[-1]:\n",
    "                self.best_function_value.append(particles[-1].best_function_value[-1])\n",
    "                self.best_position.append(particles[-1].best_position[-1])\n",
    "\n",
    "\n",
    "        self.best_position = [self.best_position[-1]]\n",
    "        self.best_function_value = [self.best_function_value[-1]]\n",
    "\n",
    "        return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1afb8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##running model with optimization function\n",
    "##1) define forward function\n",
    "##2) call Pso with swarmsize = input shape of input attribute of forward function in model\n",
    "\n",
    "'''model = ConvNetQuake()\n",
    "#model.cuda() ##compute engine device\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=[0])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-4)\n",
    "criterion = nn.BCELoss()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac8b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch-pso  ##check with built in torch pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f477874e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Invalid function handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m pso \u001b[38;5;241m=\u001b[39m Pso(swarmsize\u001b[38;5;241m=\u001b[39mbatch_size,maxiter\u001b[38;5;241m=\u001b[39mnum_iters)\n\u001b[0;32m     12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(batch_x)\n\u001b[1;32m---> 13\u001b[0m bp,value \u001b[38;5;241m=\u001b[39m \u001b[43mpso\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m##run model and get optimized outputs\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, batch_y)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m##bp === is loss\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m##value , v === is accuracy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[146], line 48\u001b[0m, in \u001b[0;36mPso.run\u001b[1;34m(self, function, lower_bound, upper_bound, kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     47\u001b[0m objective_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: function(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(function, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid function handle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lower_bound) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(upper_bound), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid bounds length\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     52\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lower_bound)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Invalid function handle"
     ]
    }
   ],
   "source": [
    "'''num_iters = 50000\n",
    "batch_size = 10\n",
    "\n",
    "acc_values = []\n",
    "acc_values_train = []\n",
    "\n",
    "for iters in range(num_iters):\n",
    "\n",
    "    batch_x, batch_y = get_batch(batch_size, split='train')  ##get train and test batches\n",
    "    ###initialize PSO with forward input\n",
    "    pso = Pso(swarmsize=batch_size,maxiter=num_iters)\n",
    "    y_pred = model(batch_x)\n",
    "    bp,value = pso.run(model(batch_x),[1,2,2,2],[16,8,4,4]) ##run model and get optimized outputs\n",
    "    loss = criterion(y_pred, batch_y)\n",
    "    ##bp === is loss\n",
    "    ##value , v === is accuracy\n",
    "    v = model(bp)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ###\n",
    "    #y_pred = model(batch_x) ##run model on input of healthy and unhealthy patients\n",
    "    #loss = criterion(y_pred, batch_y)\n",
    "    #optimizer.zero_grad()\n",
    "    #loss.backward()\n",
    "    #optimizer.step()\n",
    "    # validation\n",
    "    if iters%100 == 0 and iters != 0:\n",
    "\n",
    "        print('Loss/train', loss, iters)\n",
    "        print('PSO Loss/train', bp, iters)\n",
    "        #writer.add_scalar('Loss/train', loss, iters)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # test_set\n",
    "            iterations = 100\n",
    "            avg_acc = 0\n",
    "\n",
    "            for _ in range(iterations):\n",
    "                batch_x, batch_y = get_batch(batch_size, split='val')\n",
    "                cleaned = model(batch_x)\n",
    "\n",
    "                count = 0\n",
    "                acc = 0\n",
    "                for num in cleaned:\n",
    "                    if int(round(num)) == int(round(batch_y[count])):\n",
    "                        acc += 10\n",
    "                    count += 1\n",
    "                avg_acc += acc\n",
    "\n",
    "            acc_values.append((avg_acc / iterations))\n",
    "            print('Accuracy/val', (avg_acc / iterations), iters)\n",
    "            #writer.add_scalar('Accuracy/val', (avg_acc / iterations), iters)\n",
    "\n",
    "            # train_set\n",
    "            iterations = 100\n",
    "            avg_acc = 0\n",
    "\n",
    "            for _ in range(iterations):\n",
    "                batch_x, batch_y = get_batch(batch_size, split='train')\n",
    "                cleaned = model(batch_x)\n",
    "\n",
    "                count = 0\n",
    "                acc = 0\n",
    "                for num in cleaned:\n",
    "                    if int(round(num)) == int(round(batch_y[count])):\n",
    "                        acc += 10\n",
    "                    count += 1\n",
    "                avg_acc += acc\n",
    "\n",
    "            acc_values_train.append((avg_acc / iterations))\n",
    "            #writer.add_scalar('Accuracy/train', (avg_acc / iterations), iters)\n",
    "            print('Accuracy/train', (avg_acc / iterations), iters)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb73c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def func(x):\n",
    "  n,sf,sp,l = x[0],x[1],x[2],x[3]\n",
    " \n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32,kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "  model.add(Conv2D(n, (sf, sf), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(sp, sp),strides=(l,l)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  cp = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')];\n",
    "\n",
    "  model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=0,\n",
    "            validation_data=(x_test, y_test),callbacks=cp)\n",
    "  \n",
    "  score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "  # loss, val\n",
    "  print('current config:',x,'val:',score[1])\n",
    "  return score[1]\n",
    "\n",
    "##################################################################\n",
    "pso = Pso(swarmsize=4,maxiter=14)\n",
    "# n,sf,sp,l\n",
    "bp,value = pso.run(func,[1,2,2,2],[16,8,4,4])\n",
    "\n",
    "v = func(bp);\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print('Test loss:', bp)\n",
    "print('Test accuracy:', value,v)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MyModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
