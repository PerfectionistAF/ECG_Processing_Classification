{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41c59523-0e4f-4666-9363-e13373524b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate, BatchNormalization\n",
    "import torch\n",
    "from torch import nn, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1700403f-84e5-4607-9ee5-bdb56e3c71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "pd.options.display.max_colwidth\n",
    "pd.set_option(\"display.expand_frame_repr\", True)\n",
    "pd.set_option('display.width', 1000)\n",
    "np.set_printoptions(threshold=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04975828-fdc8-44f6-aee7-ad39e4d5e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  51.0  1.0  4.0     140.0  299.0  0.0      0.0    173.0    1.0      1.6   \n",
      "1  59.0  1.0  4.0     110.0  239.0  0.0      2.0    142.0    1.0      1.2   \n",
      "2  70.0  1.0  4.0     130.0  322.0  0.0      2.0    109.0    0.0      2.4   \n",
      "3  52.0  1.0  2.0     128.0  205.0  1.0      0.0    184.0    0.0      0.0   \n",
      "4  60.0  1.0  4.0     145.0  282.0  0.0      2.0    142.0    1.0      2.8   \n",
      "\n",
      "   slope   ca  thal  num age_category  total_risk  exercise_angina  \\\n",
      "0    1.0  0.0   7.0    1  Middle-aged       439.0             True   \n",
      "1    2.0  1.0   7.0    1  Middle-aged       349.0            False   \n",
      "2    2.0  3.0   3.0    1       Senior       452.0            False   \n",
      "3    1.0  0.0   3.0    0  Middle-aged       333.0            False   \n",
      "4    2.0  2.0   7.0    1       Senior       427.0            False   \n",
      "\n",
      "   cholesterol_hdl_ratio  \n",
      "0               1.728324  \n",
      "1               1.683099  \n",
      "2               2.954128  \n",
      "3               1.114130  \n",
      "4               1.985915  \n",
      "Training Accuracy: 0.98\n",
      "Testing Accuracy: 0.96\n",
      "\n",
      "Confusion Matrix training:\n",
      "[[249   2]\n",
      " [  9 198]]\n",
      "\n",
      "Confusion Matrix testing:\n",
      "[[61  2]\n",
      " [ 3 49]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        63\n",
      "           1       0.96      0.94      0.95        52\n",
      "\n",
      "    accuracy                           0.96       115\n",
      "   macro avg       0.96      0.96      0.96       115\n",
      "weighted avg       0.96      0.96      0.96       115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##generate labels from risk factor on example dataset\n",
    "%run E:/Jupyter/A_DNN/risk_factors/risk_factors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b2f686-a32c-4d32-99e9-7429c81f39ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "      <th>age_category</th>\n",
       "      <th>total_risk</th>\n",
       "      <th>exercise_angina</th>\n",
       "      <th>cholesterol_hdl_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Middle-aged</td>\n",
       "      <td>267.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.18254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "421  49.0  1.0  3.0     118.0  149.0  0.0      2.0    126.0    0.0      0.8   \n",
       "\n",
       "     slope   ca  thal  num age_category  total_risk  exercise_angina  \\\n",
       "421    1.0  3.0   3.0    1  Middle-aged       267.0            False   \n",
       "\n",
       "     cholesterol_hdl_ratio  \n",
       "421                1.18254  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_data = df\n",
    "table_data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9ad938-97dc-4686-a0e0-6fe285715599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(573, 18)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b58f567-b89c-4dff-aa2f-484daf0b19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate labels from ecg on example dataset\n",
    "#%run E:/Jupyter/A_DNN/convnetquake-pso.ipynb  ##uncomment to retrain and visualise\n",
    "%run E:/VSprojects/ECG_Processing_Classification/ECG_Processing_Classification/mi_diagnosis/ecg/convnetquake-pso.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b881b737-8979-4063-925f-aba0b70bd4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DataParallel(\n",
       "  (module): ConvNetQuake(\n",
       "    (conv1): Conv1d(3, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv3): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv4): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv5): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv6): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv7): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv8): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (linear1): Linear(in_features=1280, out_features=128, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Load DataParallel object model.pth\n",
    "model.parameters ##test model from imported .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d695eb-7d21-4a8c-99cb-da353bfd6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"models\")\n",
    "##test labels from entire model\n",
    "##load pytorch file containing pretrained model\n",
    "loaded_model_2 = torch.load('./models/convnet-pso_multi_arch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3010f9e6-3d0d-4ba2-8de0-0d61d09eb8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parallel.data_parallel.DataParallel"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_model_2) ##stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8f0f93-cab8-4c84-b2d2-96ec3eeb2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check PyTorch dict on previous DataParallel instance from convnetquake-pso.ipynb\n",
    "loaded_model_2_states = model.load_state_dict(torch.load('./models/convnet-pso_multi_state.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c48c12-7bec-4145-9543-e7dd1d20df96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_2_states  ##stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32116a2-cc0c-4c9e-ba5b-3e8ccc61fbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ii</th>\n",
       "      <th>v6</th>\n",
       "      <th>vz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>[ 0.326   0.324   0.321  ... -0.0435 -0.041  -...</td>\n",
       "      <td>[-0.06   -0.054  -0.0475 ... -0.035  -0.035  -...</td>\n",
       "      <td>[-0.333  -0.335  -0.3395 ...  0.0075  0.0085  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                                 ii                                                 v6                                                 vz\n",
       "59          59  [ 0.326   0.324   0.321  ... -0.0435 -0.041  -...  [-0.06   -0.054  -0.0475 ... -0.035  -0.035  -...  [-0.333  -0.335  -0.3395 ...  0.0075  0.0085  ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##test diagnosis on loaded model\n",
    "##from ptb_data_prepare.ipynb, healthy_train_signals.csv\n",
    "#%run E:/Jupyter/A_DNN/ptb_data_prepare.ipynb\n",
    "file_path = \"E:/Jupyter/A_DNN/healthy_train_signals.csv\"\n",
    "ecg_data = pd.read_csv(file_path)\n",
    "ecg_data.sample()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5161d0c1-c431-4eb2-bab2-7221c5140234",
   "metadata": {},
   "source": [
    "#V6 EXAMPLE\n",
    "'''array([0.458 , 0.458 , 0.457 , 0.4555, 0.453 , 0.45  , 0.449 , 0.45  ,\n",
    "       0.448 , 0.4455, 0.4465, 0.4505, 0.4515, 0.4535, 0.4585, 0.4605,\n",
    "       0.4655, 0.4715, 0.4715, 0.4715, 0.4765, 0.4795, 0.4795, 0.482 ,\n",
    "       0.487 , 0.4875, 0.485 , 0.483 , 0.4805, 0.479 , 0.4795, 0.482 ,\n",
    "       0.483 , 0.4795, 0.4785, 0.476 , 0.472 , 0.473 , 0.4785, 0.484 ,\n",
    "       0.4825, 0.4785, 0.4725, 0.4625, 0.459 , 0.457 , 0.4535, 0.4565,\n",
    "       0.4605, 0.465 , 0.4725, 0.477 , 0.481 ])'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af0263eb-c83f-4a54-961e-20580d31550b",
   "metadata": {},
   "source": [
    "#VZ EXAMPLE\n",
    "'''array([-0.1025, -0.1   , -0.0925, -0.0885, -0.0905, -0.0905, -0.0895,\n",
    "       -0.085 , -0.08  , -0.0825, -0.0835, -0.067 , -0.064 , -0.0665,\n",
    "       -0.064 , -0.0685, -0.0695, -0.082 , -0.0865, -0.085 , -0.089 ,\n",
    "       -0.0855, -0.087 , -0.091 , -0.091 , -0.092 , -0.09  , -0.0805,\n",
    "       -0.0745, -0.075 , -0.0705, -0.07  , -0.081 , -0.086 , -0.0935,\n",
    "       -0.105 , -0.0935, -0.08  , -0.0865, -0.0795, -0.073 , -0.0835,\n",
    "       -0.0895, -0.0915, -0.0885, -0.0925, -0.1015, -0.105 , -0.105 ,\n",
    "       -0.1005, -0.1035, -0.109 , -0.113 ])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ca475d-303a-4017-92a1-c3861aaaf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#print(ecg_data['ii'][18])\n",
    "#pd.options.display.max_colwidth\n",
    "ecg_test =[[-0.1245, -0.1275, -0.1305, -0.134 , -0.134 , -0.1345, -0.1345,\n",
    "       -0.1345, -0.132 , -0.1295, -0.128 , -0.1225, -0.121 , -0.123 ,\n",
    "       -0.128 , -0.132 , -0.13  , -0.127 , -0.13  , -0.133 , -0.1305,\n",
    "       -0.1235, -0.1205, -0.121 , -0.121 , -0.1205, -0.1205, -0.122 ,\n",
    "       -0.123 , -0.1215, -0.1165, -0.1125, -0.1145, -0.117 , -0.1175,\n",
    "       -0.115 , -0.105 , -0.1   , -0.1095, -0.114 , -0.114 , -0.1145,\n",
    "       -0.1115, -0.104 , -0.103 , -0.1075, -0.1105, -0.114 , -0.1135,\n",
    "       -0.1115, -0.108 , -0.107 , -0.114 ],\n",
    "           [0.458 , 0.458 , 0.457 , 0.4555, 0.453 , 0.45  , 0.449 , 0.45  ,\n",
    "       0.448 , 0.4455, 0.4465, 0.4505, 0.4515, 0.4535, 0.4585, 0.4605,\n",
    "       0.4655, 0.4715, 0.4715, 0.4715, 0.4765, 0.4795, 0.4795, 0.482 ,\n",
    "       0.487 , 0.4875, 0.485 , 0.483 , 0.4805, 0.479 , 0.4795, 0.482 ,\n",
    "       0.483 , 0.4795, 0.4785, 0.476 , 0.472 , 0.473 , 0.4785, 0.484 ,\n",
    "       0.4825, 0.4785, 0.4725, 0.4625, 0.459 , 0.457 , 0.4535, 0.4565,\n",
    "       0.4605, 0.465 , 0.4725, 0.477 , 0.481 ],\n",
    "           [-0.1025, -0.1   , -0.0925, -0.0885, -0.0905, -0.0905, -0.0895,\n",
    "       -0.085 , -0.08  , -0.0825, -0.0835, -0.067 , -0.064 , -0.0665,\n",
    "       -0.064 , -0.0685, -0.0695, -0.082 , -0.0865, -0.085 , -0.089 ,\n",
    "       -0.0855, -0.087 , -0.091 , -0.091 , -0.092 , -0.09  , -0.0805,\n",
    "       -0.0745, -0.075 , -0.0705, -0.07  , -0.081 , -0.086 , -0.0935,\n",
    "       -0.105 , -0.0935, -0.08  , -0.0865, -0.0795, -0.073 , -0.0835,\n",
    "       -0.0895, -0.0915, -0.0885, -0.0925, -0.1015, -0.105 , -0.105 ,\n",
    "       -0.1005, -0.1035, -0.109 , -0.113 ]]\n",
    "ecg_tensor = torch.tensor(ecg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17df6fff-084d-4290-a736-3ccc2bd15ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ecg_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a756d6c9-bcfa-4384-9a70-b79d4ee8c8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(ecg_tensor[2])  ##stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de82a563-18d7-4261-83bd-a69bd4e2e7e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ConvNetQuake(\n",
       "    (conv1): Conv1d(3, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv3): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv4): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv5): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv6): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv7): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (conv8): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (linear1): Linear(in_features=1280, out_features=128, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89170e60-ea53-4396-8cf2-04c31f6074d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of running_mean: torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "#########DEBUGGING######\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "bn_layer = nn.BatchNorm1d(32)\n",
    "bn_layer.running_mean = nn.Parameter(torch.zeros(27))\n",
    "print(\"Size of running_mean:\", bn_layer.running_mean.size())\n",
    "#########DEBUGGING######"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b94c85a-70c1-4451-bcdb-2654a7b3ac43",
   "metadata": {},
   "source": [
    "[-0.1245, -0.1275, -0.1305, -0.1340, -0.1340, -0.1345, -0.1345, -0.1345,\n",
    "        -0.1320, -0.1295, -0.1280, -0.1225, -0.1210, -0.1230, -0.1280, -0.1320,\n",
    "        -0.1300, -0.1270, -0.1300, -0.1330, -0.1305, -0.1235, -0.1205, -0.1210,\n",
    "        -0.1210, -0.1205, -0.1205]\n",
    "[0.4580, 0.4580, 0.4570, 0.4555, 0.4530, 0.4500, 0.4490, 0.4500, 0.4480,\n",
    "        0.4455, 0.4465, 0.4505, 0.4515, 0.4535, 0.4585, 0.4605, 0.4655, 0.4715,\n",
    "        0.4715, 0.4715, 0.4765, 0.4795, 0.4795, 0.4820, 0.4870, 0.4875, 0.4850]\n",
    "[-0.1025, -0.1000, -0.0925, -0.0885, -0.0905, -0.0905, -0.0895, -0.0850,\n",
    "        -0.0800, -0.0825, -0.0835, -0.0670, -0.0640, -0.0665, -0.0640, -0.0685,\n",
    "        -0.0695, -0.0820, -0.0865, -0.0850, -0.0890, -0.0855, -0.0870, -0.0910,\n",
    "        -0.0910, -0.0920, -0.0900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3674ee39-9c84-4999-a4c7-b67964ab6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in loaded_model_2.named_parameters():\n",
    "    if 'running_mean' in name:\n",
    "        param.data = param.data[:27]  # Set running_mean to 27 then rerun on the ecg_tensor\n",
    "        param.requires_grad = False\n",
    "\n",
    "torch.save(model, './models/modified_convnetquake_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5fbc9f0f-f340-4fa9-b1b9-a9c3db5d11b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parallel.data_parallel.DataParallel"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_2_modified = torch.load('./models/modified_convnetquake_model.pth')\n",
    "type(loaded_model_2_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "becbbd28-d221-43ae-b21d-f199ccd78999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loaded_model_2_modified.parameters\n",
    "#loaded_model_2_modified.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d8990fc-b4b1-4ba2-92d0-0c4a0ea9ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#ecg_pred = loaded_model_2_modified(ecg_tensor_truncate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55789cfe-f243-442b-9288-7f7dcb057b0f",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a custom tensor with 27 elements\n",
    "custom_running_mean = nn.Parameter(ecg_tensor_truncate)\n",
    "\n",
    "# Define a BatchNorm1d layer with 32 features\n",
    "bn_layer = nn.BatchNorm3d(num_features=32)\n",
    "\n",
    "# Set the running_mean to the custom tensor\n",
    "bn_layer.running_mean = custom_running_mean\n",
    "\n",
    "# Verify the size of the running_mean\n",
    "print(\"Size of running_mean:\", bn_layer.running_mean.size())\n",
    "output_tensor = bn_layer(ecg_tensor_truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84188acb-7188-4e0e-8b1a-9850675e6684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1245, -0.1275, -0.1305, -0.1340, -0.1340, -0.1345, -0.1345, -0.1345,\n",
       "         -0.1320, -0.1295, -0.1280, -0.1225, -0.1210, -0.1230, -0.1280, -0.1320,\n",
       "         -0.1300, -0.1270, -0.1300, -0.1330, -0.1305, -0.1235, -0.1205, -0.1210,\n",
       "         -0.1210, -0.1205, -0.1205],\n",
       "        [ 0.4580,  0.4580,  0.4570,  0.4555,  0.4530,  0.4500,  0.4490,  0.4500,\n",
       "          0.4480,  0.4455,  0.4465,  0.4505,  0.4515,  0.4535,  0.4585,  0.4605,\n",
       "          0.4655,  0.4715,  0.4715,  0.4715,  0.4765,  0.4795,  0.4795,  0.4820,\n",
       "          0.4870,  0.4875,  0.4850],\n",
       "        [-0.1025, -0.1000, -0.0925, -0.0885, -0.0905, -0.0905, -0.0895, -0.0850,\n",
       "         -0.0800, -0.0825, -0.0835, -0.0670, -0.0640, -0.0665, -0.0640, -0.0685,\n",
       "         -0.0695, -0.0820, -0.0865, -0.0850, -0.0890, -0.0855, -0.0870, -0.0910,\n",
       "         -0.0910, -0.0920, -0.0900]], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg_tensor_tuncate = [[-0.1245, -0.1275, -0.1305, -0.1340, -0.1340, -0.1345, -0.1345, -0.1345,\n",
    "        -0.1320, -0.1295, -0.1280, -0.1225, -0.1210, -0.1230, -0.1280, -0.1320,\n",
    "        -0.1300, -0.1270, -0.1300, -0.1330, -0.1305, -0.1235, -0.1205, -0.1210,\n",
    "        -0.1210, -0.1205, -0.1205], \n",
    "                      [0.4580, 0.4580, 0.4570, 0.4555, 0.4530, 0.4500, 0.4490, 0.4500, 0.4480,\n",
    "        0.4455, 0.4465, 0.4505, 0.4515, 0.4535, 0.4585, 0.4605, 0.4655, 0.4715,\n",
    "        0.4715, 0.4715, 0.4765, 0.4795, 0.4795, 0.4820, 0.4870, 0.4875, 0.4850], \n",
    "                      [-0.1025, -0.1000, -0.0925, -0.0885, -0.0905, -0.0905, -0.0895, -0.0850,\n",
    "        -0.0800, -0.0825, -0.0835, -0.0670, -0.0640, -0.0665, -0.0640, -0.0685,\n",
    "        -0.0695, -0.0820, -0.0865, -0.0850, -0.0890, -0.0855, -0.0870, -0.0910,\n",
    "        -0.0910, -0.0920, -0.0900]]\n",
    "ecg_tensor_truncate = torch.tensor(ecg_tensor_tuncate)\n",
    "nn.Parameter(ecg_tensor_truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0bc9b0a-80bc-46ad-9720-bbe52c73a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 32, 32, 32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "batchnorm_layers = [loaded_model_2.module.bn1, loaded_model_2.module.bn2, loaded_model_2.module.bn3, loaded_model_2.module.bn4\n",
    "                   , loaded_model_2.module.bn5, loaded_model_2.module.bn6, loaded_model_2.module.bn7, loaded_model_2.module.bn8]\n",
    "#batchnorm_layers  ##stable\n",
    "new_ele = 27\n",
    "##current list of running_mean elements\n",
    "current_ele = []\n",
    "for i in range(0, len(batchnorm_layers)):\n",
    "    current_ele.append(batchnorm_layers[i].running_mean.size(0))\n",
    "    \n",
    "print(current_ele) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f1288f5-a12a-44ac-86f7-2423bc1377f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.batchnorm.BatchNorm1d"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batchnorm_layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdaff2a6-5430-4c44-8a9f-ea08a6298d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batchnorm_layers)):\n",
    "    batchnorm_layers[i].running_mean = nn.Parameter(torch.full_like(batchnorm_layers[i].running_mean, 27.0))\n",
    "    batchnorm_layers[i].running_mean.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a762796f-19f3-4301-b6aa-d70d641ba886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ea328-8a23-4ba7-b8ab-60e9dd935a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if elements != current_num_elements:\n",
    "    # Resize or manipulate the tensor as needed to achieve the desired number of elements\n",
    "    # Example: resizing the tensor\n",
    "    batchnorm_layer.running_mean = nn.Parameter(torch.nn.functional.interpolate(batch_norm_layer.running_mean.unsqueeze(0), size=(desired_num_elements,)).squeeze(0))\n",
    "\n",
    "# Freeze the running_mean tensor\n",
    "batch_norm_layer.running_mean.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a20702c7-0748-4f17-a2d3-f9080eefffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm = nn.BatchNorm1d(num_features=27)\n",
    "output_tensor = batch_norm(ecg_tensor_truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e51730a-b849-4e87-9e61-ccd63a89ff5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 14 elements not 32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ecg_pred \u001b[38;5;241m=\u001b[39m loaded_model_2(output_tensor)\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[1;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\VSprojects\\ECG_Processing_Classification\\ECG_Processing_Classification\\mi_diagnosis\\ecg\\convnetquake-pso.py:56\u001b[0m, in \u001b[0;36mConvNetQuake.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):  \u001b[38;5;66;03m##x is preliminary input\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(F\u001b[38;5;241m.\u001b[39mrelu((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))))\n\u001b[0;32m     57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(F\u001b[38;5;241m.\u001b[39mrelu((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[0;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(F\u001b[38;5;241m.\u001b[39mrelu((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    184\u001b[0m     bn_training,\n\u001b[0;32m    185\u001b[0m     exponential_average_factor,\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    187\u001b[0m )\n",
      "File \u001b[1;32mE:\\Downloads\\ANACONDA3\\ANACONDA3\\Lib\\site-packages\\torch\\nn\\functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2510\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[0;32m   2511\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 14 elements not 32"
     ]
    }
   ],
   "source": [
    "#with torch.no_grad():\n",
    "ecg_pred = loaded_model_2(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebc227-633c-47f1-8fac-8ccf4fcc32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.round(ecg_pred)\n",
    "predictions_np = predictions.numpy()\n",
    "print(\"Predictions:\", predictions_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab51c64f-cad4-456d-8d62-f1649b8d6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "##force running_mean to be 27 for all ecg inputs\n",
    "##Dense layer\n",
    "##BatchNorm num_features 27\n",
    "##Flatten tensor\n",
    "signal_input = Input(shape=ecg_tensor.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4c70a13-19cb-41d9-b658-8b3470a4daa3",
   "metadata": {},
   "source": [
    "signal_branch = Dense(27*32, activation='relu')(signal_input)\n",
    "signal_branch = nn.BatchNorm1d(27)(signal_branch)\n",
    "signal_branch = torch.reshape(signal_branch, (27, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "877d1e19-a3bd-4e64-a471-5820fd56586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = Input(shape=(224,16))#Input(shape=(224, 224))\n",
    "table_input = Input(shape=(224,16))#Input(shape=(576,18))   ##Image segement 1/14 + all features  ##loop over segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eb9634f-cad3-46a8-9ec7-49e75a95a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture for each modality\n",
    "###TEST DIMENSIONS\n",
    "image_branch = Dense(256, activation='relu')(image_input)\n",
    "table_branch = Dense(256, activation='relu')(table_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4abd1eb3-450a-419b-9fb9-fe98b0097e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat dense layers after both are tarined on their respective cnns\n",
    "fusion_layer = Concatenate()([image_branch, table_branch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ffb0996-c16a-4041-a285-c936cbed54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_layer = Dense(128, activation='relu')(fusion_layer)\n",
    "output_layer = Dense(2, activation='softmax')(fusion_layer)  ##2 output classes, healthy or unhealthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ba7687d-009a-4079-81ce-53393abe931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[image_input, table_input], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55cc019c-42a9-4cc6-bdf2-cc9d456476fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7e797-1d91-477e-a1d8-be8f95f1e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([images, texts], labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5adea-f514-4906-b939-11a88585bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layers for each modality\n",
    "#image_input = Input(shape=(224, 224, 3))  # Example input shape for images\n",
    "#text_input = Input(shape=(100,))  # Example input shape for text\n",
    "\n",
    "# Define the network architecture for each modality\n",
    "#image_branch = Dense(256, activation='relu')(image_input)\n",
    "#text_branch = Dense(256, activation='relu')(text_input)\n",
    "\n",
    "# Concatenate the outputs of the modalities\n",
    "#fusion_layer = Concatenate()([image_branch, text_branch])\n",
    "\n",
    "# Add additional layers for fusion and prediction\n",
    "#fusion_layer = Dense(128, activation='relu')(fusion_layer)\n",
    "#output_layer = Dense(num_classes, activation='softmax')(fusion_layer)  # Replace 'num_classes' with the number of output classes\n",
    "\n",
    "# Create the multi-modal deep fusion model\n",
    "#model = Model(inputs=[image_input, text_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "#model.fit([images, texts], labels, epochs=10, batch_size=32)  # Replace 'images', 'texts', and 'labels' with your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87cee35-445a-4a3a-993d-e2607697470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 34.285714285714285 640.0\n",
      "0.0 34.285714285714285 34.285714285714285 640.0\n",
      "0.0 68.57142857142857 34.285714285714285 640.0\n",
      "0.0 102.85714285714286 34.285714285714285 640.0\n",
      "0.0 137.14285714285714 34.285714285714285 640.0\n",
      "0.0 171.42857142857142 34.285714285714285 640.0\n",
      "0.0 205.71428571428572 34.285714285714285 640.0\n",
      "0.0 240.0 34.285714285714285 640.0\n",
      "0.0 274.2857142857143 34.285714285714285 640.0\n",
      "0.0 308.57142857142856 34.285714285714285 640.0\n",
      "0.0 342.85714285714283 34.285714285714285 640.0\n",
      "0.0 377.1428571428571 34.285714285714285 640.0\n",
      "0.0 411.42857142857144 34.285714285714285 640.0\n",
      "0.0 445.7142857142857 34.285714285714285 640.0\n"
     ]
    }
   ],
   "source": [
    "##remove index column on copy of dataframe and take all columns except age column to get 224x16\n",
    "##divide the 224 image height into 14 images 224x16\n",
    "import cv2,time\n",
    "\n",
    "img = cv2.imread('E:/Jupyter/A_DNN/ptb_ecg_filtered/0_multiple_ Myocardial infarction.png')\n",
    "img2 = img\n",
    "\n",
    "height, width, channels = img.shape\n",
    "# Number of pieces Horizontally \n",
    "W_SIZE  = 1 \n",
    "# Number of pieces Vertically to each Horizontal  \n",
    "H_SIZE = 14\n",
    "\n",
    "for ih in range(H_SIZE ):\n",
    "   for iw in range(W_SIZE ):\n",
    "   \n",
    "      x = width/W_SIZE * iw \n",
    "      y = height/H_SIZE * ih\n",
    "      h = (height / H_SIZE)\n",
    "      w = (width / W_SIZE )\n",
    "      print(x,y,h,w)\n",
    "      img = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "      NAME = str(time.time()) \n",
    "      cv2.imwrite(\"output_images/\" + str(ih)+str(iw) +  \".png\",img)\n",
    "      img = img2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
